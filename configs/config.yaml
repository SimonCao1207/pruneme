model_path: huzama/Full-3.2-16L
model_name: Full-3.2-16L
num_layers: 16
dataset_name: pico-lm/pretokenized-dolma 
revision: pico-epoch_0
batch_size: 16
max_length: 2048
device: cuda
prune_layers: [6]
method: prune-multiple
evaluators:
  - label: boolq_mc_5shot 
  - label: boolq_val_mc_5shot
  - label: hellaswag_mc_5shot
  - label: piqa_mc_5shot
  - label: openbookqa_mc_5shot
  - label: winogrande_mc_5shot
  - label: arc_easy_mc_5shot
  - label: csqa_mc_5shot