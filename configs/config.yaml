model_path: huzama/Full-3.2-16L
model_name: Full-3.2-16L
num_layers: 16
dataset_name: pico-lm/pretokenized-dolma 
revision: pico-epoch_0
batch_size: 16
max_length: 2048
device: cuda
prune_layers: [6]
method : prune-multiple